{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prospective job data project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data project was to use the training and testing data to draw insight and make predictions of whether a business was bad or not.  There were a number of features provided.  Some were defined as discrete or continuous while others were undefined.  The response variable was binomial (0 or 1) and indicated whether a business was bad or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### First I used the training set provided to do some exploratory work to understand the data better and to identify the best statistical model for prediction.  I used Tableau to look at the relationship between the features and our outcome variable of \"IsBad\".   I found that almost 20% of the training set were \"Bad\" and a little over 80% were \"Not Bad\".  Because the number of bad and not bad businesses are not equal, I used frequency distributions to better understand the relationships between the features and IsBad.\n",
    "\n",
    "### I looked at both the Risk Score and Quality Score to (1) determine whether it was discrete or continuous and (2)  evaluate whether there was a relationship between the Score features and IsBad.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scores](Scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I saw that the Score features both appear to be approximately normally distributed and therefore are continuous variables.  There also appears to be a relationship between both Score features and IsBad as businesses that are bad seem to have both lower Risk Scores and Quality Scores.\n",
    "\n",
    "### Next, I looked at the number of reviews and the number of positive reviews, which were both defined as continuous variables.  Again, I see that there appears to be a relationship between both features and IsBad.  If a business is bad, it is more likely to have less positive reviews and overall number of reviews.\n",
    "\n",
    "### Feature Engineering:  Having the number of positive reviews and the overall number of reviews allowed me to calculate the percentage of reviews that were positive.  Again, there was an apparent relationship between % of positive reviews and IsBad in that bad businesses were more likely to have a lower frequency of positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Business Reviews](Reviews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I looked at the Number of Citations, another continuous variable, and Bad status.  Because most businesses had zero citations, I limited the y axis to 20% of businesses so that the relationship between number of citations (greater than 0) and IsBad could be better understood visually.  Because there were some extreme outliers in number of citations, I also limited the number of citations to less than 90.\n",
    "\n",
    "###  Businesses that are bad seem to have a relationship with a higher number of citations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Citations](Citations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I also looked at the relationship between the discrete Risk Type features and IsBad.  I made a couple of observations.\n",
    "### 1. There were a few businesses that had Risk Type values of -1 or -2.  These seemed suspicious and are most likely place holders for NaNs and were changed to \"Nothing\" in the data analysis.\n",
    "### 2. There were a few categories withing the Risk Type features that had a clear relationship with IsBad, such as Economic Trajectory Risk 2 and 5.  The categories of Ownership Risk had a clear relationship while the categories of Previous Ownership Risk and Type of Business Risk seemed to have less of a relationship with IsBad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Risk](Risk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Indices variables were not described as either discrete or continuous so I first graphed the distribution of the number of businesses by index category to get a better understanding of the data.  The Ability and Willingness Indices look like they could be exponentially distributed but the Stability Index looks like it is a discrete variable.  Without clearer evidence that they had continuous distributions, I decided to treat the Indices features as discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Index Distributions](Index Distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### After determining that the Indices features were discrete, I explored the relationship between the features and IsBad.  I saw that there were some differences between some of the categories and IsBad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Indices](Indices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data to make a prediction on test data\n",
    "\n",
    "### My next step was to use the training data set to test prediction algorithms to find the algorithm with the highest accuracy as determined with ROC score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from numpy import arange\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bin_categorical(df, cutoffPercent = .01):\n",
    "    \"\"\"Lumps categorcal features with frequency < cutoffPercent into 'Other'.\"\"\"\n",
    "    for col in df:\n",
    "        sizes = df[col].value_counts(normalize = True)\n",
    "        # get the names of the levels that make up less than 1% of the dataset\n",
    "        values_to_delete = sizes[sizes<cutoffPercent].index\n",
    "        df[col].ix[df[col].isin(values_to_delete)] = \"Other\"\n",
    "    return df\n",
    "    \n",
    "\n",
    "def get_binary_values(data_frame):\n",
    "   \"\"\"encodes cateogrical features in Pandas.\n",
    "   \"\"\"\n",
    "   all_columns = pandas.DataFrame( index = data_frame.index)\n",
    "   for col in data_frame.columns:\n",
    "       data = pandas.get_dummies(data_frame[col], prefix=col.encode('ascii', 'replace'))\n",
    "       all_columns = pandas.concat([all_columns, data], axis=1)\n",
    "   return all_columns\n",
    "\n",
    "\n",
    "def find_zero_var(df):\n",
    "   \"\"\"finds columns in the dataframe with zero variance -- ie those\n",
    "       with the same value in every observation.\n",
    "   \"\"\"   \n",
    "   toKeep = []\n",
    "   toDelete = []\n",
    "   for col in df:\n",
    "       if len(df[col].value_counts()) > 1:\n",
    "           toKeep.append(col)\n",
    "       else:\n",
    "           toDelete.append(col)\n",
    "       ##\n",
    "   return {'toKeep':toKeep, 'toDelete':toDelete} \n",
    "\n",
    "\n",
    "def find_perfect_corr(df):\n",
    "   \"\"\"finds columns that are eother positively or negatively perfectly correlated (with correlations of +1 or -1), and creates a dict \n",
    "       that includes which columns to drop so that each remaining column\n",
    "       is independent\n",
    "   \"\"\"  \n",
    "   corrMatrix = df.corr()\n",
    "   corrMatrix.loc[:,:] =  numpy.tril(corrMatrix.values, k = -1)\n",
    "   already_in = set()\n",
    "   result = []\n",
    "   for col in corrMatrix:\n",
    "       perfect_corr = corrMatrix[col][abs(numpy.round(corrMatrix[col],10)) >= .9].index.tolist()\n",
    "       if perfect_corr and col not in already_in:\n",
    "           already_in.update(set(perfect_corr))\n",
    "           perfect_corr.append(col)\n",
    "           result.append(perfect_corr)\n",
    "   toRemove = []\n",
    "   for item in result:\n",
    "       toRemove.append(item[1:(len(item)+1)])\n",
    "   toRemove = sum(toRemove, [])\n",
    "   return {'corrGroupings':result, 'toRemove':toRemove}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "### I replaced data that seemed to be placeholders for NaNs as Nans, changed data type to string for the categorical variables and integer for the continuous variables, and defined the PercentPositive feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change data that seems suspicious to NANs\n",
    "df = df.replace(-1, numpy.nan)\n",
    "df = df.replace(-2, numpy.nan)\n",
    "df['Index-Ability'] = df['Index-Ability'].replace(0, numpy.nan)\n",
    "df['Index-Willingness'] = df['Index-Willingness'].replace(0, numpy.nan)\n",
    "df['Index-Stability'] = df['Index-Stability'].replace(0, numpy.nan)\n",
    "\n",
    "# Change data types\n",
    "## Setting index variables as strings\n",
    "df[['Id', 'Risk-TypeOfBusiness', 'Risk-Ownership', 'Risk-EconomicTrajectory', 'Risk-PreviousOwnership', 'Index-Ability', 'Index-Stability', 'Index-Willingness']] = df[['Id', 'Risk-TypeOfBusiness', 'Risk-Ownership', 'Risk-EconomicTrajectory', 'Risk-PreviousOwnership', 'Index-Ability', 'Index-Stability', 'Index-Willingness']].astype(str)\n",
    "df[['RiskScore', 'QualityScore', 'Complaints', 'NumberOfReviews', 'PositiveReviews', 'Citations']] = df[['RiskScore', 'QualityScore', 'Complaints', 'NumberOfReviews', 'PositiveReviews', 'Citations']].astype(float)\n",
    "df['isBad'] = df[ 'isBad'].astype(int)\n",
    "\n",
    "# Feature Engineering - Create a new variable based on percent of positive reviews\n",
    "df['PercentPositive'] = df.PositiveReviews / df.NumberOfReviews\n",
    "\n",
    "# Drop any rows with all NA's\n",
    "df.dropna(how = 'all', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up explanatory and response (IsBad) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkraunz/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RiskScore</th>\n",
       "      <th>QualityScore</th>\n",
       "      <th>Complaints</th>\n",
       "      <th>NumberOfReviews</th>\n",
       "      <th>PositiveReviews</th>\n",
       "      <th>Citations</th>\n",
       "      <th>PercentPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3214.00000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3214.000000</td>\n",
       "      <td>3213.000000</td>\n",
       "      <td>3212.000000</td>\n",
       "      <td>3213.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.62654</td>\n",
       "      <td>67.201120</td>\n",
       "      <td>1.451261</td>\n",
       "      <td>15.672682</td>\n",
       "      <td>1.643324</td>\n",
       "      <td>5.477273</td>\n",
       "      <td>0.081301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.93353</td>\n",
       "      <td>6.280727</td>\n",
       "      <td>3.339610</td>\n",
       "      <td>13.267621</td>\n",
       "      <td>2.941588</td>\n",
       "      <td>14.188956</td>\n",
       "      <td>0.126138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.10000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58.90000</td>\n",
       "      <td>62.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.75000</td>\n",
       "      <td>66.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>62.60000</td>\n",
       "      <td>70.700000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67.10000</td>\n",
       "      <td>98.400000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RiskScore  QualityScore   Complaints  NumberOfReviews  \\\n",
       "count  3214.00000   3214.000000  3211.000000      3214.000000   \n",
       "mean     60.62654     67.201120     1.451261        15.672682   \n",
       "std       2.93353      6.280727     3.339610        13.267621   \n",
       "min      50.10000     55.000000     0.000000         1.000000   \n",
       "25%      58.90000     62.600000     0.000000         5.000000   \n",
       "50%      60.75000     66.300000     0.000000        12.000000   \n",
       "75%      62.60000     70.700000     2.000000        22.000000   \n",
       "max      67.10000     98.400000    59.000000        93.000000   \n",
       "\n",
       "       PositiveReviews    Citations  PercentPositive  \n",
       "count      3213.000000  3212.000000      3213.000000  \n",
       "mean          1.643324     5.477273         0.081301  \n",
       "std           2.941588    14.188956         0.126138  \n",
       "min           0.000000     0.000000         0.000000  \n",
       "25%           0.000000     0.000000         0.000000  \n",
       "50%           0.000000     0.000000         0.000000  \n",
       "75%           2.000000     5.000000         0.125000  \n",
       "max          25.000000   328.000000         0.909091  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('Id', axis=1, inplace=True)\n",
    "explanatory_features = [col for col in df.columns if col not in ['isBad']]\n",
    "explanatory_df = df[explanatory_features]\n",
    "\n",
    "explanatory_df.dropna(how = 'all', inplace = True)\n",
    "\n",
    "explanatory_col_names = explanatory_df.columns\n",
    "\n",
    "response_series = df.isBad\n",
    "response_series.dropna(how = 'all', inplace = True)\n",
    "response_series.index[~response_series.index.isin(explanatory_df.index)]\n",
    "\n",
    "explanatory_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I described the numerical data so I could have an understanding of the distribution and whether or not there were missing values.\n",
    "## Prepare data for prediction model algorithms\n",
    "### I processed the data by filling numerical features with NaNs with the median value of the data set, filling categorical features with NaNs with \"Nothing\", binning low frequency categorical variables (less than 1%) into an \"Other\" category, changing categorical features into binary variables, and evaluating the features for those with zero variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toKeep': ['RiskScore', 'QualityScore', 'Complaints', 'NumberOfReviews', 'PositiveReviews', 'Citations', 'PercentPositive', 'Risk-TypeOfBusiness_1.0', 'Risk-TypeOfBusiness_3.0', 'Risk-TypeOfBusiness_Other', 'Risk-Ownership_0.0', 'Risk-Ownership_1.0', 'Risk-Ownership_2.0', 'Risk-Ownership_3.0', 'Risk-Ownership_4.0', 'Risk-Ownership_Other', 'Risk-EconomicTrajectory_0.0', 'Risk-EconomicTrajectory_1.0', 'Risk-EconomicTrajectory_2.0', 'Risk-EconomicTrajectory_3.0', 'Risk-EconomicTrajectory_4.0', 'Risk-EconomicTrajectory_5.0', 'Risk-EconomicTrajectory_6.0', 'Risk-EconomicTrajectory_Other', 'Risk-PreviousOwnership_0.0', 'Risk-PreviousOwnership_1.0', 'Risk-PreviousOwnership_2.0', 'Risk-PreviousOwnership_5.0', 'Risk-PreviousOwnership_Other', 'Index-Ability_1.0', 'Index-Ability_2.0', 'Index-Ability_3.0', 'Index-Ability_4.0', 'Index-Ability_5.0', 'Index-Ability_6.0', 'Index-Ability_7.0', 'Index-Ability_8.0', 'Index-Ability_9.0', 'Index-Ability_Other', 'Index-Stability_1.0', 'Index-Stability_2.0', 'Index-Stability_3.0', 'Index-Stability_4.0', 'Index-Stability_5.0', 'Index-Stability_6.0', 'Index-Stability_7.0', 'Index-Stability_8.0', 'Index-Stability_9.0', 'Index-Stability_Other', 'Index-Willingness_1.0', 'Index-Willingness_2.0', 'Index-Willingness_3.0', 'Index-Willingness_4.0', 'Index-Willingness_5.0', 'Index-Willingness_6.0', 'Index-Willingness_7.0', 'Index-Willingness_8.0', 'Index-Willingness_Other'], 'toDelete': []}\n"
     ]
    }
   ],
   "source": [
    "# 1. SPLIT DATA INTO CATEGORICAL AND NUMERIC DATA\n",
    "\n",
    "string_features = explanatory_df.ix[:, explanatory_df.dtypes == 'object']\n",
    "numeric_features = explanatory_df.ix[:, explanatory_df.dtypes != 'object']\n",
    "\n",
    "# 2. FILL NUMERIC NANS THROUGH IMPUTATION\n",
    "\n",
    "imputer = Imputer(missing_values='NaN', strategy='median',axis=0)\n",
    "imputer.fit(numeric_features)\n",
    "\n",
    "# impute and put back into a dataframe with the columns and indices from before\n",
    "numeric_features = pandas.DataFrame(imputer.transform(numeric_features), columns=numeric_features.columns, index=numeric_features.index)\n",
    "\n",
    "# 3. FILL CATEGORICAL NANS WITH 'NOTHING'\n",
    "\n",
    "string_features = string_features.fillna('Nothing')\n",
    "\n",
    "# 4. DETECT LOW-FREQUENCY LEVELS IN CATEGORICAL FEATURES AND BIN THEM UNDER 'OTHER'\n",
    "\n",
    "string_features = bin_categorical(string_features)\n",
    "\n",
    "# grab categories for use with the out-of-sample data\n",
    "orig_categories = {}\n",
    "for col in string_features.columns:\n",
    "    orig_categories[col] = string_features[col].unique()\n",
    "\n",
    "# 5. ENCODE EACH CATEGORICAL VARIABLE INTO A SEQUENCE OF BINARY VARIABLES\n",
    "\n",
    "encoded_string_features  =  get_binary_values(string_features)\n",
    "\n",
    "# 6. MERGE ENCODED CATEGORICAL DATA WITH YOUR NUMERIC DATA\n",
    "\n",
    "explanatory_df = pandas.concat([numeric_features, encoded_string_features], axis=1)\n",
    "\n",
    "# 7. REMOVE FEATURES WITH NO VARIATION\n",
    "\n",
    "keep_delete = find_zero_var(explanatory_df)\n",
    "print keep_delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No features had zero variation.\n",
    "\n",
    "### Next, I checked features for correlation of greater than 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corrGroupings': [['Risk-TypeOfBusiness_3.0',\n",
       "   'Risk-Ownership_1.0',\n",
       "   'Risk-TypeOfBusiness_1.0'],\n",
       "  ['Risk-EconomicTrajectory_Other', 'Risk-Ownership_Other'],\n",
       "  ['Index-Stability_Other', 'Index-Ability_Other']],\n",
       " 'toRemove': ['Risk-Ownership_1.0',\n",
       "  'Risk-TypeOfBusiness_1.0',\n",
       "  'Risk-Ownership_Other',\n",
       "  'Index-Ability_Other']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. REMOVE CORRELATED FEATURES\n",
    "\n",
    "find_perfect_corr(explanatory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I found four sets of features that were had a correlation greater than 0.9.  I deleted one feature in each pair.  I then scaled each feature to a mean of zero and variance of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explanatory_df.drop('Risk-Ownership_1.0', axis=1, inplace=True)\n",
    "explanatory_df.drop('Risk-TypeOfBusiness_1.0', axis=1, inplace=True)\n",
    "explanatory_df.drop('Risk-Ownership_Other', axis=1, inplace=True)\n",
    "explanatory_df.drop('Index-Ability_Other', axis=1, inplace=True)\n",
    "\n",
    "# 9. SCALE DATA WITH ZERO MEAN AND UNIT VARIANCE\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(explanatory_df)\n",
    "explanatory_df = pandas.DataFrame(scaler.transform(explanatory_df), columns = explanatory_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "### I decided to use F1 scores to identify the best predictive model of Random Forests, Gradient Boosting Trees, Logistic Regression, and Extra Trees.  I chose F1 because I wanted to use a score that valued true positivity (or recall).  I performed a 10-fold cross validation for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 Score: 0.230642716806\n",
      "Gradient Boosting Tree F1 Score: 0.274569510047\n",
      "Logistic Regression F1 Score: 0.282241777999\n",
      "Extra Trees F1 Score: 0.210989962494\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators = 500)\n",
    "f1_scores_rf = cross_val_score(rf, explanatory_df, response_series, cv=10, scoring = 'f1', n_jobs = -1)\n",
    "\n",
    "#Gradient Boosting Tree\n",
    "boosting_tree = GradientBoostingClassifier()\n",
    "f1_scores_gbm = cross_val_score(boosting_tree, explanatory_df, response_series, cv=10, scoring = 'f1', n_jobs = -1)\n",
    "\n",
    "# Neural Network\n",
    "logistic_classifier = LogisticRegression()\n",
    "f1_scores_lr = cross_val_score(logistic_classifier, explanatory_df, response_series, cv=10, scoring = 'f1')\n",
    "\n",
    "# Extra Trees Classifier\n",
    "et = ExtraTreesClassifier(n_estimators = 500)\n",
    "f1_scores_et = cross_val_score(et, explanatory_df, response_series, cv=10, scoring = 'f1', n_jobs = -1)\n",
    "\n",
    "print \"Random Forest F1 Score:\", f1_scores_rf.mean()\n",
    "print \"Gradient Boosting Tree F1 Score:\", f1_scores_gbm.mean()\n",
    "print \"Logistic Regression F1 Score:\", f1_scores_lr.mean()\n",
    "print \"Extra Trees F1 Score:\", f1_scores_et.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Regression model had the highest F1 score and I chose to move forward with model refinement with it.\n",
    "\n",
    "### Before I performed recursive feature elimination, I wanted to see what the sensitivity (detecting true positives) and specificity (detecting true negatives) were.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.825762289981\n",
      "Predicted Label     0    1   All\n",
      "True Label                      \n",
      "0                2528   58  2586\n",
      "1                 502  126   628\n",
      "All              3030  184  3214\n"
     ]
    }
   ],
   "source": [
    "pred_bad = logistic_classifier.predict(explanatory_df)\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "number_correct = len(response_series[response_series == pred_bad])\n",
    "total = len(response_series)\n",
    "accuracy = number_correct / total\n",
    "\n",
    "print \"Accuracy: \", accuracy\n",
    "\n",
    "cm = pandas.crosstab(response_series, pred_bad, rownames=['True Label'], colnames=['Predicted Label'], margins=True)\n",
    "\n",
    "print cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity: 126/628 = 20.1%\n",
    "### Specificity: 2528/2586 = 97.8%\n",
    "### Observation: Good at identifying what is not a bad business but not great at identifying what is a good business but overall is 82.6% accurate.\n",
    "\n",
    "## Recursive Feature Elimination (RFE)\n",
    "### The above models had many features due to the large amount of categorical features.  I chose to move forward with the Logistic Regression model and to determine which features could be eliminated because they didn't contribute to increasing the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features :39 of 54 considered\n",
      "[ 0.01548077  0.1709488   0.1667351   0.21208463  0.22343225  0.22940177\n",
      "  0.23658647  0.23422217  0.25473265  0.24922697  0.26111623  0.26435289\n",
      "  0.26451332  0.27093006  0.27420062  0.2771072   0.27367101  0.27403102\n",
      "  0.27329401  0.27321674  0.27823115  0.27596951  0.27208229  0.27654582\n",
      "  0.27502672  0.2798535   0.27779476  0.27787118  0.27573738  0.27595913\n",
      "  0.27557686  0.27728418  0.27953613  0.27753458  0.28043309  0.28043309\n",
      "  0.28403218  0.28425938  0.28473982  0.28473982  0.28133677  0.2771417\n",
      "  0.27988609  0.27953041  0.27953041  0.28298073  0.28258928  0.28224178\n",
      "  0.28224178  0.28224178  0.28224178  0.28224178  0.28224178  0.28224178]\n",
      "Index([u'RiskScore', u'QualityScore', u'Complaints', u'NumberOfReviews',\n",
      "       u'Citations', u'PercentPositive', u'Risk-TypeOfBusiness_Other',\n",
      "       u'Risk-Ownership_0.0', u'Risk-Ownership_2.0', u'Risk-Ownership_4.0',\n",
      "       u'Risk-EconomicTrajectory_1.0', u'Risk-EconomicTrajectory_2.0',\n",
      "       u'Risk-EconomicTrajectory_3.0', u'Risk-EconomicTrajectory_6.0',\n",
      "       u'Risk-EconomicTrajectory_Other', u'Risk-PreviousOwnership_0.0',\n",
      "       u'Risk-PreviousOwnership_1.0', u'Risk-PreviousOwnership_2.0',\n",
      "       u'Risk-PreviousOwnership_5.0', u'Risk-PreviousOwnership_Other',\n",
      "       u'Index-Ability_1.0', u'Index-Ability_3.0', u'Index-Ability_4.0',\n",
      "       u'Index-Ability_5.0', u'Index-Ability_6.0', u'Index-Ability_7.0',\n",
      "       u'Index-Ability_9.0', u'Index-Stability_1.0', u'Index-Stability_2.0',\n",
      "       u'Index-Stability_5.0', u'Index-Stability_6.0', u'Index-Stability_7.0',\n",
      "       u'Index-Stability_Other', u'Index-Willingness_1.0',\n",
      "       u'Index-Willingness_4.0', u'Index-Willingness_5.0',\n",
      "       u'Index-Willingness_7.0', u'Index-Willingness_8.0',\n",
      "       u'Index-Willingness_Other'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimkraunz/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrAQYuuYQTdwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10837aa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## set up the estimator\n",
    "rfe_cv = RFECV(estimator=logistic_classifier, step=1, cv=10,\n",
    "              scoring='f1', verbose = 1)\n",
    "rfe_cv.fit(explanatory_df, response_series)\n",
    "\n",
    "print \"Optimal number of features :{0} of {1} considered\".format(rfe_cv.n_features_,len(explanatory_df.columns))\n",
    "print rfe_cv.grid_scores_\n",
    "\n",
    "# pull out the features used\n",
    "features_used = explanatory_df.columns[rfe_cv.get_support()]\n",
    "print features_used  \n",
    "\n",
    "## plot out the results\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (F1)\")\n",
    "plt.plot(range(1, len(rfe_cv.grid_scores_) + 1), rfe_cv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I identified the features that would give the highest accuracy moving forward without diluting the Logistic Regression model.  I did not see a large increase in the F1 scorea after around 18 features were used.\n",
    "\n",
    "### Note: I was curious because some of the features identified in RFE did not seem as important as others that were eliminated.  I ran the Logistic Regression Model with only the features that had an importance greater than 1% and got roughly the same F1 score.\n",
    "\n",
    "## **Grid Search to identify optimal parameters**\n",
    "### I performed a grid search to identify the learning rate, subsampling rate and number of estimators that would give me the highest accuracy for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001}\n",
      "0.32604999092\n"
     ]
    }
   ],
   "source": [
    "# Use only features indentified in RFE\n",
    "\n",
    "best_features = explanatory_df[features_used]\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "lr_grid = GridSearchCV(LogisticRegression(penalty='l2'), param_grid, scoring = 'f1')\n",
    "GridSearchCV(cv=None,\n",
    "       estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,\n",
    "          penalty='l2', tol=0.0001),\n",
    "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n",
    "\n",
    "lr_grid.fit(best_features, response_series)\n",
    "\n",
    "print lr_grid.best_params_\n",
    "print lr_grid.best_score_\n",
    "\n",
    "# PULL OUT THE WINNING ESTIMATOR\n",
    "best_lr_grid = lr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With an optimized model, I wanted to see how the sensitivity and specificity had changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.824206596142\n",
      "Predicted Label     0    1   All\n",
      "True Label                      \n",
      "0                2514   72  2586\n",
      "1                 493  135   628\n",
      "All              3007  207  3214\n"
     ]
    }
   ],
   "source": [
    "pred_bad = best_lr_grid.predict(best_features)\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "number_correct = len(response_series[response_series == pred_bad])\n",
    "total = len(response_series)\n",
    "accuracy = number_correct / total\n",
    "\n",
    "print \"Accuracy: \", accuracy\n",
    "\n",
    "cm = pandas.crosstab(response_series, pred_bad, rownames=['True Label'], colnames=['Predicted Label'], margins=True)\n",
    "\n",
    "print cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My accuracy stayed roughly the same at 82.4%.  Unfortunately it is a small increase from the baseline accuracy of 80.4%\n",
    "### Sensitivity: 135/628 = 21.5%\n",
    "### Specificity: 72/2514 = 97.2%\n",
    "###  With model optimization, I was able to increase the model's ability to identify bad businesses with a slight decrease in it's ability to identify businesses that aren't bad.\n",
    "\n",
    "## Making predictions to the test data\n",
    "### I then read in the test data and repeated the above data manipulation and processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import testing df\n",
    "t_df = pandas.read_csv(\"test.csv\")\n",
    "test_df = t_df\n",
    "\n",
    "############# DATA MANIPULATION  ###################\n",
    "\n",
    "# Change data that seems suspicious to NANs\n",
    "test_df = test_df.replace(-1, numpy.nan)\n",
    "test_df = test_df.replace(-2, numpy.nan)\n",
    "test_df['Index-Ability'] = test_df['Index-Ability'].replace(0, numpy.nan)\n",
    "test_df['Index-Willingness'] = test_df['Index-Willingness'].replace(0, numpy.nan)\n",
    "test_df['Index-Stability'] = test_df['Index-Stability'].replace(0, numpy.nan)\n",
    "\n",
    "# Change data types\n",
    "## Setting index variables as strings\n",
    "test_df[['Id', 'Risk-TypeOfBusiness', 'Risk-Ownership', 'Risk-EconomicTrajectory', 'Risk-PreviousOwnership', 'Index-Ability', 'Index-Stability', 'Index-Willingness']] = test_df[['Id', 'Risk-TypeOfBusiness', 'Risk-Ownership', 'Risk-EconomicTrajectory', 'Risk-PreviousOwnership', 'Index-Ability', 'Index-Stability', 'Index-Willingness']].astype(str)\n",
    "test_df[['RiskScore', 'QualityScore', 'Complaints', 'NumberOfReviews', 'PositiveReviews', 'Citations']] = test_df[['RiskScore', 'QualityScore', 'Complaints', 'NumberOfReviews', 'PositiveReviews', 'Citations']].astype(float)\n",
    "\n",
    "# Created the PercentPostive feature\n",
    "test_df['PercentPositive'] = test_df.PositiveReviews / test_df.NumberOfReviews\n",
    "\n",
    "# Drop any rows with all NA's\n",
    "test_df.dropna(how = 'all', inplace = True)\n",
    "\n",
    "# Set up explanatory and response features\n",
    "test_df.drop('Id', axis=1, inplace=True)\n",
    "test_explanatory_df = test_df\n",
    "\n",
    "test_explanatory_df.dropna(how = 'all', inplace = True)\n",
    "\n",
    "test_explanatory_col_names = test_explanatory_df.columns\n",
    "\n",
    "test_explanatory_df.describe()\n",
    "\n",
    "# 1. SPLIT DATA INTO CATEGORICAL AND NUMERIC DATA\n",
    "\n",
    "test_string_features = test_explanatory_df.ix[:, test_explanatory_df.dtypes == 'object']\n",
    "test_numeric_features = test_explanatory_df.ix[:, test_explanatory_df.dtypes != 'object']\n",
    "\n",
    "\n",
    "# 2. FILL NUMERIC NANS THROUGH IMPUTATION\n",
    "\n",
    "test_numeric_features = pandas.DataFrame(imputer.transform(test_numeric_features), columns = test_numeric_features.columns)\n",
    "\n",
    "# 3. FILL CATEGORICAL NANS WITH 'NOTHING'\n",
    "\n",
    "test_string_features = test_string_features.fillna('Nothing')\n",
    "\n",
    "# for each string feature, if there is a value that is not in the original dataset, make it 'other'.\n",
    "for col in test_string_features:\n",
    "    test_string_features[col].ix[~test_string_features[col].isin(orig_categories[col])] = \"Other\"\n",
    "\n",
    "test_encoded_string_features  =  get_binary_values(test_string_features)\n",
    "\n",
    "# post-encoding, add any dummy features that were in the original\n",
    "for col in encoded_string_features:\n",
    "    if col not in test_encoded_string_features:\n",
    "        test_encoded_string_features[col] = 0\n",
    "\n",
    "\n",
    "# REORDER COLUMNS TO MATCH ORIGINAL\n",
    "\n",
    "test_encoded_string_features = test_encoded_string_features[encoded_string_features.columns]\n",
    "\n",
    "# COMBINE NUMERIC AND STRING FEATURES\n",
    "test_explanatory_df = pandas.concat([test_numeric_features, test_encoded_string_features], axis=1)\n",
    "\n",
    "# REMOVE CORELATED FEATURES\n",
    "  \n",
    "test_explanatory_df.drop('Risk-Ownership_1.0', axis=1, inplace=True)\n",
    "test_explanatory_df.drop('Risk-TypeOfBusiness_1.0', axis=1, inplace=True)\n",
    "test_explanatory_df.drop('Risk-Ownership_Other', axis=1, inplace=True)\n",
    "test_explanatory_df.drop('Index-Ability_Other', axis=1, inplace=True)\n",
    "\n",
    "# SCALE\n",
    "test_explanatory_df = pandas.DataFrame(scaler.transform(test_explanatory_df),columns=test_explanatory_df.columns,index=test_explanatory_df.index)\n",
    "\n",
    "# LIMIT EXPLANATORY DATAFRAME TO BEST FEATURES (FROM RFE)\n",
    "\n",
    "test_best_features = test_explanatory_df[features_used]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the trained Logistic Regression model to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST DATA USING OPTIMIZED LOGISTIC REGRESSION MODEL DATA\n",
    "\n",
    "pred_test = pandas.DataFrame(best_lr_grid.predict(test_best_features), columns = ['IsBad'])\n",
    "\n",
    "pred_test_df = pandas.concat([t_df.Id, pred_test], axis=1)\n",
    "\n",
    "# WRITE PREDICTIONS TO CSV\n",
    "pred_test_df.to_csv('kraunz_test_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions and observations\n",
    "###  I was able to identify the best classifier model using F1 scoring, which values sensitivity and specificity, as the Logistic Regression model.  I refined the model using recursive feature elimination and grid search to optimize it.  The accuracy of the final model is decent and the specificity is high but the sensitivity could still be improved (~20%), especially since it was a modest increase in accuracy over the baseline accuracy.  Interestingly, almost 20% of the training data set were \"Bad\", while only a little over 7% of the testing data set were predicted to be \"Bad\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
